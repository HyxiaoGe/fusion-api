# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## è¯­è¨€å’Œå¼€å‘è§„èŒƒ

### è¯­è¨€è¦æ±‚

- **æ‰€æœ‰å›å¤å¿…é¡»ä½¿ç”¨ä¸­æ–‡**ï¼šåŒ…æ‹¬ä»£ç æ³¨é‡Šã€è§£é‡Šè¯´æ˜ã€é”™è¯¯ä¿¡æ¯ç­‰
- **Git æäº¤ä¿¡æ¯å¿…é¡»ä½¿ç”¨ä¸­æ–‡**ï¼šæäº¤æ ‡é¢˜å’Œæè¿°éƒ½ä½¿ç”¨ä¸­æ–‡
- **æ–‡æ¡£å’Œæ³¨é‡Šä½¿ç”¨ä¸­æ–‡**ï¼šæ‰€æœ‰æ–°åˆ›å»ºçš„æ–‡æ¡£ã€ä»£ç æ³¨é‡Šéƒ½ä½¿ç”¨ä¸­æ–‡

### Git æäº¤è§„èŒƒ

- **æäº¤ä¿¡æ¯æ ¼å¼è¦æ±‚**ï¼š

  ```
  feat: æ·»åŠ RAGæ£€ç´¢å¢å¼ºåŠŸèƒ½
  
  - å®ç°å‘é‡æ•°æ®åº“é›†æˆ
  - ä¼˜åŒ–æ–‡æ¡£åˆ†å—ç­–ç•¥
  - æ·»åŠ æ··åˆæœç´¢æ”¯æŒ
  
  ğŸ¤– Generated with [Claude Code](https://claude.ai/code)
  
  Co-Authored-By: Claude <noreply@anthropic.com>
  ```

- **å¿…é¡»åŒ…å« Co-author ä¿¡æ¯**ï¼šæ¯ä¸ªæäº¤éƒ½è¦åŒ…å« `Co-authored-by: Claude Code <noreply@anthropic.com>`

- **ä½¿ç”¨ä¸­æ–‡æäº¤ç±»å‹**ï¼š
  - `feat`: æ–°åŠŸèƒ½
  - `fix`: ä¿®å¤bug
  - `docs`: æ–‡æ¡£æ›´æ–°
  - `style`: ä»£ç æ ¼å¼è°ƒæ•´
  - `refactor`: é‡æ„ä»£ç 
  - `test`: æµ‹è¯•ç›¸å…³
  - `chore`: æ„å»ºå·¥å…·æˆ–è¾…åŠ©å·¥å…·çš„å˜åŠ¨

### ä¸ªäººå¼€å‘åå¥½

- **ä»£ç é£æ ¼**ï¼šä½¿ç”¨4ä¸ªç©ºæ ¼ç¼©è¿›ï¼Œä¸ä½¿ç”¨Tab
- **å‡½æ•°å‘½å**ï¼šä½¿ç”¨åŠ¨è¯å¼€å¤´ï¼Œå¦‚ `è·å–ç”¨æˆ·ä¿¡æ¯()`, `å¤„ç†æ–‡æ¡£()`
- **é”™è¯¯å¤„ç†**ï¼šä¼˜å…ˆä½¿ç”¨ try-exceptï¼Œæä¾›ä¸­æ–‡é”™è¯¯ä¿¡æ¯
- **æ—¥å¿—æ ¼å¼**ï¼šä½¿ç”¨ä¸­æ–‡æ—¥å¿—ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
- **æ³¨é‡Šè¯­è¨€**ï¼šæ‰€æœ‰ä»£ç æ³¨é‡Šä½¿ç”¨ä¸­æ–‡
- **å˜é‡å‘½å**ï¼šä½¿ç”¨è‹±æ–‡ï¼Œä½†æ³¨é‡Šè¯´æ˜ä½¿ç”¨ä¸­æ–‡
- **å‡½æ•°è®¾è®¡**ï¼šå•ä¸ªå‡½æ•°ä¸è¶…è¿‡50è¡Œï¼ŒèŒè´£å•ä¸€
- **å¯¼å…¥é¡ºåº**ï¼šæ ‡å‡†åº“ â†’ ç¬¬ä¸‰æ–¹åº“ â†’ æœ¬åœ°æ¨¡å—ï¼Œæ¯ç»„ä¹‹é—´ç©ºä¸€è¡Œ
- **å­—ç¬¦ä¸²å¤„ç†**ï¼šä¼˜å…ˆä½¿ç”¨ f-string æ ¼å¼åŒ–ï¼Œé¿å…ä½¿ç”¨ % æ ¼å¼åŒ–
- **æ–‡ä»¶è·¯å¾„**ï¼šä½¿ç”¨ `pathlib.Path` è€Œä¸æ˜¯ `os.path`
- **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨ `.env` æ–‡ä»¶ç®¡ç†ç¯å¢ƒå˜é‡ï¼Œæ•æ„Ÿä¿¡æ¯ä¸å†™å…¥ä»£ç 
- **ä¾èµ–ç®¡ç†**ï¼šä½¿ç”¨ `requirements.txt` é”å®šç‰ˆæœ¬ï¼Œé‡è¦ä¾èµ–æ·»åŠ ä¸­æ–‡æ³¨é‡Šè¯´æ˜ç”¨é€”

### æ–‡æ¡£å’Œæ³¨é‡Šåå¥½

- **å‡½æ•°æ–‡æ¡£**ï¼šæ‰€æœ‰å‡½æ•°å¿…é¡»æœ‰ä¸­æ–‡docstringï¼Œè¯´æ˜å‚æ•°ã€è¿”å›å€¼ã€å¼‚å¸¸
- **ç±»æ–‡æ¡£**ï¼šç±»çš„ä½œç”¨ã€ä¸»è¦æ–¹æ³•ã€ä½¿ç”¨ç¤ºä¾‹éƒ½ç”¨ä¸­æ–‡æè¿°
- **å¤æ‚é€»è¾‘**ï¼šè¶…è¿‡5è¡Œçš„å¤æ‚é€»è¾‘å¿…é¡»æ·»åŠ ä¸­æ–‡æ³¨é‡Šè§£é‡Š
- **TODOæ ‡è®°**ï¼šä½¿ç”¨ä¸­æ–‡ `# TODO: å¾…å®ç°åŠŸèƒ½æè¿°` æ ¼å¼
- **ä»£ç ç¤ºä¾‹**ï¼šåœ¨æ–‡æ¡£ä¸­æä¾›ä¸­æ–‡æ³¨é‡Šçš„å®Œæ•´ä»£ç ç¤ºä¾‹

### æµ‹è¯•å’Œè´¨é‡ä¿è¯

- **æµ‹è¯•è¦†ç›–**ï¼šé‡è¦å‡½æ•°å¿…é¡»æœ‰å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹
- **æµ‹è¯•å‘½å**ï¼šæµ‹è¯•å‡½æ•°ä½¿ç”¨ä¸­æ–‡æè¿°ï¼Œå¦‚ `test_ç”¨æˆ·ç™»å½•_æˆåŠŸåœºæ™¯()`
- **æ–­è¨€ä¿¡æ¯**ï¼šæ–­è¨€å¤±è´¥æ—¶æä¾›ä¸­æ–‡é”™è¯¯ä¿¡æ¯
- **æµ‹è¯•æ•°æ®**ï¼šä½¿ç”¨ä¸­æ–‡æµ‹è¯•æ•°æ®ï¼Œæ›´è´´è¿‘å®é™…ä½¿ç”¨åœºæ™¯
- **æ€§èƒ½æµ‹è¯•**ï¼šå…³é”®ç®—æ³•éœ€è¦æ·»åŠ æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•

### è°ƒè¯•å’Œæ—¥å¿—åå¥½

- **è°ƒè¯•ä¿¡æ¯**ï¼šä½¿ç”¨ä¸­æ–‡debugä¿¡æ¯ï¼Œä¾¿äºå®šä½é—®é¢˜
- **æ—¥å¿—çº§åˆ«**ï¼šå¼€å‘ç¯å¢ƒä½¿ç”¨DEBUGï¼Œç”Ÿäº§ç¯å¢ƒä½¿ç”¨INFO
- **å¼‚å¸¸æ•è·**ï¼šæ•è·å¼‚å¸¸æ—¶è®°å½•ä¸­æ–‡ä¸Šä¸‹æ–‡ä¿¡æ¯
- **æ‰“å°è°ƒè¯•**ï¼šä¸´æ—¶è°ƒè¯•å¯ä»¥ä½¿ç”¨printï¼Œä½†æ­£å¼ä»£ç å¿…é¡»ä½¿ç”¨logging
- **é”™è¯¯è¿½è¸ª**ï¼šé‡è¦é”™è¯¯å¿…é¡»è®°å½•å®Œæ•´çš„ä¸­æ–‡é”™è¯¯å †æ ˆ

### å®‰å…¨å’Œæ€§èƒ½åå¥½

- **è¾“å…¥éªŒè¯**ï¼šæ‰€æœ‰å¤–éƒ¨è¾“å…¥å¿…é¡»éªŒè¯ï¼Œæä¾›ä¸­æ–‡é”™è¯¯æç¤º
- **å¯†ç å¤„ç†**ï¼šä½¿ç”¨bcryptç­‰å®‰å…¨ç®—æ³•ï¼Œä¸æ˜æ–‡å­˜å‚¨
- **APIé™æµ**ï¼šé‡è¦æ¥å£æ·»åŠ é€Ÿç‡é™åˆ¶
- **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
- **èµ„æºæ¸…ç†**ï¼šåŠæ—¶å…³é—­æ–‡ä»¶ã€æ•°æ®åº“è¿æ¥ç­‰èµ„æº

### é¡¹ç›®ç»“æ„åå¥½

- **ç›®å½•å‘½å**ï¼šä½¿ç”¨ä¸­æ–‡æ‹¼éŸ³æˆ–è‹±æ–‡ï¼Œé¿å…ä¸­æ–‡ç›®å½•å
- **æ–‡ä»¶åˆ†ç±»**ï¼šå·¥å…·å‡½æ•°æ”¾åœ¨ `utils/`ï¼Œé…ç½®æ–‡ä»¶æ”¾åœ¨ `config/`
- **æ¨¡å—åˆ’åˆ†**ï¼šæŒ‰åŠŸèƒ½æ¨¡å—åˆ’åˆ†ï¼Œæ¯ä¸ªæ¨¡å—èŒè´£æ¸…æ™°
- **å¸¸é‡å®šä¹‰**ï¼šæ‰€æœ‰é­”æ³•æ•°å­—å’Œå­—ç¬¦ä¸²å®šä¹‰ä¸ºæœ‰æ„ä¹‰çš„å¸¸é‡
- **ç¯å¢ƒéš”ç¦»**ï¼šå¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒä¸¥æ ¼éš”ç¦»

## Project Overview

Fusion API is a Python-based AI chat integration platform built with FastAPI that provides a unified interface for multiple Large Language Model (LLM) providers including Anthropic, OpenAI, Google Gemini, DeepSeek, and various Chinese AI services (Qwen, Wenxin, Hunyuan, etc.).

## Key Commands

### Development
```bash
# Install dependencies
pip install -r requirements.txt

# Run development server with hot reload
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# Run with Docker
docker-compose up -d

# View Docker logs
docker-compose logs -f

# Rebuild and restart Docker containers
docker-compose build && docker-compose up -d
```

### Production
```bash
# Run production server (4 workers)
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

## Architecture Overview

The application follows a clean architecture pattern with clear separation of concerns:

### Core Components

1. **AI Integration Layer** (`app/ai/`)
   - `llm_manager.py`: Central manager for all LLM integrations
   - Provider-specific adapters in `providers/` subdirectory
   - Each provider adapter implements a common interface for chat completions

2. **API Layer** (`app/api/`)
   - FastAPI routers for different resource endpoints
   - Main endpoints: chat, files, settings, prompts, models, credentials
   - Authentication endpoints for OAuth flows

3. **Service Layer** (`app/services/`)
   - Business logic separated from API layer
   - Key services:
     - `conversation_service.py`: Manages chat sessions and history
     - `file_service.py`: Handles file uploads and processing
     - `auto_title_service.py`: Generates conversation titles using LLMs
     - `hot_topic_service.py`: Manages trending topics
     - `scheduled_task_service.py`: Background task scheduling

4. **Data Layer** (`app/db/`)
   - SQLAlchemy models for PostgreSQL
   - Repository pattern for data access
   - Models: Conversation, Message, File, Setting, User, etc.

5. **Vector Search** (`app/services/chromadb_service.py`)
   - ChromaDB integration for semantic search
   - Embeddings storage and retrieval
   - RAG (Retrieval Augmented Generation) support

### Key Design Patterns

1. **Adapter Pattern**: Each LLM provider has an adapter implementing a common interface
2. **Repository Pattern**: Database access is abstracted through repository classes
3. **Dependency Injection**: FastAPI's dependency injection for services and database sessions
4. **Middleware Pipeline**: Request timeout, CORS, and session management

### Important Implementation Details

1. **Stream Support**: All LLM integrations support streaming responses
2. **Error Handling**: Custom exceptions with proper HTTP status codes
3. **Configuration**: Environment-based configuration via `.env` file
4. **Authentication**: JWT tokens with OAuth provider support (GitHub, Google)
5. **File Processing**: Supports PDF, DOCX, and text file uploads with content extraction

### Database Schema

The application uses PostgreSQL with the following main tables:
- `conversations`: Chat sessions with metadata
- `messages`: Individual messages in conversations
- `files`: Uploaded files with vector embeddings
- `settings`: Application and user settings
- `users`: User accounts with OAuth associations
- `credentials`: Encrypted API credentials for LLM providers

### Testing

Currently, the project has minimal test infrastructure. When adding tests:
- Create test files in the `test/` directory
- Consider adding pytest to requirements.txt
- Test API endpoints using FastAPI's TestClient
- Mock external LLM API calls to avoid using real credentials

### Common Development Tasks

1. **Adding a New LLM Provider**:
   - Create a new adapter in `app/ai/providers/`
   - Implement the base adapter interface
   - Register in `LLMManager` in `app/ai/llm_manager.py`
   - Add configuration constants in `app/constants/`

2. **Adding New API Endpoints**:
   - Create router in `app/api/`
   - Implement service logic in `app/services/`
   - Add Pydantic schemas in `app/schemas/`
   - Register router in `main.py`

3. **Database Migrations**:
   - Modify models in `app/db/models/`
   - The app auto-creates tables on startup (see `app/db/base.py`)
   - For production, consider using Alembic for migrations

### Performance Considerations

1. **Docker Resource Limits**: CPU limited to 1 core, memory to 1GB
2. **Request Timeout**: 10-second timeout middleware applied
3. **Database Connections**: Connection pooling via SQLAlchemy
4. **Streaming**: Use streaming for long LLM responses to improve UX

### Security Notes

1. API credentials are encrypted before storage
2. CORS is currently open - restrict for production
3. Environment variables for sensitive configuration
4. JWT tokens for authentication with configurable expiry